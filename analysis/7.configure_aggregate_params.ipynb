{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configure Aggregate Module Params\n",
    "\n",
    "This notebook should be used as a test for ensuring correct aggregate parameters before aggregate processing.\n",
    "Cells marked with <font color='red'>SET PARAMETERS</font> contain crucial variables that need to be set according to your specific experimental setup and data organization.\n",
    "Please review and modify these variables as needed before proceeding with the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'>SET PARAMETERS</font>\n",
    "\n",
    "### Fixed parameters for aggregate module\n",
    "\n",
    "- `CONFIG_FILE_PATH`: Path to a Brieflow config file used during processing. Absolute or relative to where workflows are run from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_FILE_PATH = \"config/config.yml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from itertools import product\n",
    "import random\n",
    "\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from lib.shared.file_utils import get_filename, load_parquet_subset\n",
    "from lib.aggregate.cell_data_utils import split_cell_data, channel_combo_subset\n",
    "from lib.aggregate.cell_classification import CellClassifier\n",
    "from lib.aggregate.montage_utils import create_cell_montage, add_filenames\n",
    "from lib.aggregate.filter import (\n",
    "    query_filter,\n",
    "    perturbation_filter,\n",
    "    missing_values_filter,\n",
    "    intensity_filter,\n",
    ")\n",
    "from lib.aggregate.align import (\n",
    "    prepare_alignment_data,\n",
    "    pca_variance_plot,\n",
    "    embed_by_pca,\n",
    "    tvn_on_controls,\n",
    ")\n",
    "from lib.aggregate.aggregate import aggregate\n",
    "from lib.aggregate.eval_aggregate import (\n",
    "    nas_summary,\n",
    "    summarize_cell_data,\n",
    "    plot_feature_distributions,\n",
    ")\n",
    "from lib.shared.configuration_utils import CONFIG_FILE_HEADER\n",
    "\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'>SET PARAMETERS</font>\n",
    "\n",
    "### Testing on subset of data\n",
    "\n",
    "- `TEST_PLATE`: Plate used for testing configuration \n",
    "- `TEST_WELL_1`: First well identifier used for testing configuration\n",
    "- `TEST_WELL_2`: Second well identifier used for testing configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_PLATE = None\n",
    "TEST_WELL_1 = None\n",
    "TEST_WELL_2 = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load config file and determine root path\n",
    "with open(CONFIG_FILE_PATH, \"r\") as config_file:\n",
    "    config = yaml.safe_load(config_file)\n",
    "ROOT_FP = Path(config[\"all\"][\"root_fp\"])\n",
    "\n",
    "# Load subset of data\n",
    "# Takes ~1 minute\n",
    "merge_final_fp = (\n",
    "    ROOT_FP\n",
    "    / \"merge\"\n",
    "    / \"parquets\"\n",
    "    / get_filename({\"plate\": TEST_PLATE, \"well\": TEST_WELL_1}, \"merge_final\", \"parquet\")\n",
    ")\n",
    "cell_data = load_parquet_subset(merge_final_fp, n_rows=25000)\n",
    "\n",
    "merge_final_fp_2 = (\n",
    "    ROOT_FP\n",
    "    / \"merge\"\n",
    "    / \"parquets\"\n",
    "    / get_filename({\"plate\": TEST_PLATE, \"well\": TEST_WELL_2}, \"merge_final\", \"parquet\")\n",
    ")\n",
    "cell_data_2 = load_parquet_subset(merge_final_fp_2, n_rows=25000)\n",
    "\n",
    "cell_data = pd.concat([cell_data, cell_data_2], ignore_index=True)\n",
    "cell_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in cell_data.columns:\n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'>SET PARAMETERS</font>\n",
    "\n",
    "### Cell Data Metadata\n",
    "\n",
    "- `METADATA_COLS_FP`: Path to TSV to store metadata cols.\n",
    "- `METADATA_COLS`: Columns in cell data with metadata (use output above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "METADATA_COLS_FP = \"config/cell_data_metadata_cols.tsv\"\n",
    "METADATA_COLS = [\n",
    "    \"plate\",\n",
    "    \"well\",\n",
    "    \"tile\",\n",
    "    \"cell_0\",\n",
    "    \"i_0\",\n",
    "    \"j_0\",\n",
    "    \"site\",\n",
    "    \"cell_1\",\n",
    "    \"i_1\",\n",
    "    \"j_1\",\n",
    "    \"distance\",\n",
    "    \"fov_distance_0\",\n",
    "    \"fov_distance_1\",\n",
    "    \"sgRNA_0\",\n",
    "    \"gene_symbol_0\",\n",
    "    \"mapped_single_gene\",\n",
    "    \"channels_min\",\n",
    "    \"nucleus_i\",\n",
    "    \"nucleus_j\",\n",
    "    \"nucleus_bounds_0\",\n",
    "    \"nucleus_bounds_1\",\n",
    "    \"nucleus_bounds_2\",\n",
    "    \"nucleus_bounds_3\",\n",
    "    \"cell_i\",\n",
    "    \"cell_j\",\n",
    "    \"cell_bounds_0\",\n",
    "    \"cell_bounds_1\",\n",
    "    \"cell_bounds_2\",\n",
    "    \"cell_bounds_3\",\n",
    "    \"cytoplasm_i\",\n",
    "    \"cytoplasm_j\",\n",
    "    \"cytoplasm_bounds_0\",\n",
    "    \"cytoplasm_bounds_1\",\n",
    "    \"cytoplasm_bounds_2\",\n",
    "    \"cytoplasm_bounds_3\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(METADATA_COLS).to_csv(METADATA_COLS_FP, index=False, header=False, sep=\"\\t\")\n",
    "\n",
    "metadata, features = split_cell_data(cell_data, METADATA_COLS)\n",
    "print(metadata.shape, features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'>SET PARAMETERS</font>\n",
    "\n",
    "### Split cells into classes\n",
    "\n",
    "- `CLASSIFIER_PATH`: Path to pickled Python object that can take a cell data dataframe and output cell classes\n",
    "\n",
    "### Evaluate splitting\n",
    "\n",
    "- `COLLAPSE_COLS`: Cell data columns to collapse on when creating a summary of cell counts. This will show the number of cells in each cell class for these particular columns. Ex: `[\"sgRNA_0\", \"gene_symbol_0\"]`.\n",
    "- `MONTAGE_CHANNEL`: Channel to use for montage generation. Usually `DAPI`.\n",
    "\n",
    "**Notes**: \n",
    "- We generate cell classes for each of the classes listed in the classifier and an \"all\" class. So for a classifier that splits by mitotic or interphase the final classes will be `[\"mitotic\", \"interphase\", \"all\"]`.\n",
    "- You must import necessary packages for the classifier in this notebook and add them to `scripts/aggregate/split_datasets.py` as well. Ex `import numpy as np` if the classifier requires `numpy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASSIFIER_PATH = None\n",
    "MONTAGE_CHANNEL = None\n",
    "COLLAPSE_COLS = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = CellClassifier.load(CLASSIFIER_PATH)\n",
    "classified_metadata, classified_features = classifier.classify_cells(metadata, features)\n",
    "\n",
    "# Create config var for cell classes\n",
    "CELL_CLASSES = list(classified_metadata[\"class\"].unique())\n",
    "\n",
    "# Show cell class counts and distribution\n",
    "print(\"Cell class counts:\")\n",
    "print(classified_metadata[\"class\"].value_counts())\n",
    "\n",
    "print(\"\\nCell class confidences:\")\n",
    "classified_metadata[\"confidence\"].hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_classes = list(classified_metadata[\"class\"].unique()) + [\"all\"]\n",
    "\n",
    "classified_metadata_copy = classified_metadata.copy(deep=True)\n",
    "classified_metadata_copy = add_filenames(classified_metadata_copy, ROOT_FP)\n",
    "\n",
    "# Create a dictionary of DataFrames for each cell class\n",
    "cell_class_dfs = {\n",
    "    cell_class: classified_metadata_copy[classified_metadata_copy[\"class\"] == cell_class]\n",
    "    for cell_class in CELL_CLASSES\n",
    "}\n",
    "\n",
    "# Define sorting directions and titles\n",
    "title_templates = {\n",
    "    True: \"Lowest Confidence {cell_class} Cells - {channel}\",\n",
    "    False: \"Highest Confidence {cell_class} Cells - {channel}\",\n",
    "}\n",
    "\n",
    "# Generate montages dynamically\n",
    "montages, titles = [], []\n",
    "for cell_class, cell_df in cell_class_dfs.items():\n",
    "    for ascending in [True, False]:\n",
    "        montage = create_cell_montage(\n",
    "            cell_data=cell_df,\n",
    "            channels=config[\"phenotype\"][\"channel_names\"],\n",
    "            selection_params={\n",
    "                \"method\": \"sorted\",\n",
    "                \"sort_by\": \"confidence\",\n",
    "                \"ascending\": ascending,\n",
    "            },\n",
    "        )[MONTAGE_CHANNEL]\n",
    "        montages.append(montage)\n",
    "        titles.append(\n",
    "            title_templates[ascending].format(\n",
    "                cell_class=cell_class, channel=MONTAGE_CHANNEL\n",
    "            )\n",
    "        )\n",
    "\n",
    "# Determine figure size dynamically\n",
    "num_rows = len(CELL_CLASSES)\n",
    "fig, axes = plt.subplots(num_rows, 2, figsize=(10, 3 * num_rows))\n",
    "\n",
    "# Display montages\n",
    "for ax, title, montage in zip(axes.flat, titles, montages):\n",
    "    ax.imshow(montage, cmap=\"gray\")\n",
    "    ax.set_title(title, fontsize=14)\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "print(\"Montages of cell classes:\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Split cell data summary:\")\n",
    "summary_df = summarize_cell_data(classified_metadata, CELL_CLASSES, COLLAPSE_COLS)\n",
    "display(summary_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'>SET PARAMETERS</font>\n",
    "\n",
    "### Aggregate by channel combos\n",
    "\n",
    "- `CHANNEL_COMBOS`: Combinations of channels to aggregate by. This is a list of lists with channel names, ex `[[\"DAPI\", \"CENPA\"], [\"DAPI\", \"WGA\"]]`.\n",
    "- `AGGREGATE_COMBO_FP`: Location of aggregate combinations dataframe.\n",
    "- `TEST_CELL_CLASS`: Cell class to configure aggregate params with. Can be any of the cell classes or `all`.\n",
    "- `TEST_CHANNEL_COMBO`: Channel combo to configure aggregate params with; must be one of the channel combos. Ex `[\"DAPI\", \"CENPA\"]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHANNEL_COMBOS = None\n",
    "AGGREGATE_COMBO_FP = \"config/aggregate_combo.tsv\"\n",
    "\n",
    "TEST_CELL_CLASS = None\n",
    "TEST_CHANNEL_COMBO = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine cell classes and channel combos\n",
    "channel_combos = [\"_\".join(combo) for combo in CHANNEL_COMBOS]\n",
    "\n",
    "# Load merge wildcard combos\n",
    "MERGE_COMBO_FP = Path(config[\"merge\"][\"merge_combo_fp\"])\n",
    "merge_wildcard_combos = pd.read_csv(MERGE_COMBO_FP, sep=\"\\t\")\n",
    "\n",
    "# Generate aggregate wildcard combos\n",
    "aggregate_wildcard_combos = pd.DataFrame(\n",
    "    product(\n",
    "        merge_wildcard_combos.itertuples(index=False, name=None),\n",
    "        cell_classes,\n",
    "        channel_combos,\n",
    "    ),\n",
    "    columns=[\"plate_well\", \"cell_class\", \"channel_combo\"],\n",
    ")\n",
    "aggregate_wildcard_combos[[\"plate\", \"well\"]] = pd.DataFrame(aggregate_wildcard_combos[\"plate_well\"].tolist(), index=aggregate_wildcard_combos.index)\n",
    "aggregate_wildcard_combos = aggregate_wildcard_combos.drop(columns=\"plate_well\")\n",
    "\n",
    "# Save aggregate wildcard combos\n",
    "aggregate_wildcard_combos.to_csv(AGGREGATE_COMBO_FP, sep=\"\\t\", index=False)\n",
    "\n",
    "print(\"Aggregate wildcard combos:\")\n",
    "aggregate_wildcard_combos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset cell class\n",
    "if TEST_CELL_CLASS != \"all\":\n",
    "    cell_class_mask = classified_metadata[\"class\"] == TEST_CELL_CLASS\n",
    "    class_metadata = classified_metadata[cell_class_mask]\n",
    "    class_features = classified_features[cell_class_mask]\n",
    "else:\n",
    "    class_metadata = classified_metadata\n",
    "    class_features = classified_features\n",
    "\n",
    "# subset features\n",
    "all_channels = config[\"phenotype\"][\"channel_names\"]\n",
    "class_features = channel_combo_subset(class_features, TEST_CHANNEL_COMBO, all_channels)\n",
    "\n",
    "# copy metadata and features for later eval\n",
    "dataset_metadata = class_metadata.copy()\n",
    "dataset_features = class_features.copy()\n",
    "\n",
    "# preview metadata and features\n",
    "display(class_metadata)\n",
    "display(class_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'>SET PARAMETERS</font>\n",
    "\n",
    "### Perturbation filtering\n",
    "\n",
    "- `FILTER_QUERIES`: Queries to use for custom filtering; ex: `[\"mapped_single_gene == True\", \"cell_quality_score > 0.8\"]`. Can be left as `None` for no filtering.\n",
    "- `PERTURBATION_NAME_COL`: Name of column used to identify perturbations. This is the column that aggregation takes place on. Ex \"gene_symbol_0\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILTER_QUERIES = None\n",
    "PERTURBATION_NAME_COL = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qf_metadata, qf_features = query_filter(class_metadata, class_features, FILTER_QUERIES)\n",
    "\n",
    "pf_metadata, pf_features = perturbation_filter(\n",
    "    qf_metadata, qf_features, PERTURBATION_NAME_COL\n",
    ")\n",
    "print(f\"Unique populations: {metadata[PERTURBATION_NAME_COL].nunique()}\")\n",
    "\n",
    "summary_df, fig = nas_summary(pf_features)\n",
    "print(summary_df[summary_df[\"percent_na\"] > 0.1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'>SET PARAMETERS</font>\n",
    "\n",
    "### Missing value filtering\n",
    "\n",
    "- `DROP_COLS_THRESHOLD`: Threshold of NA values above which an entire column is dropped. Usually `0.1`\n",
    "- `DROP_ROWS_THRESHOLD`: Threshold of NA values above which an entire row is dropped. Usually `0.01`\n",
    "- `IMPUTE`: Whether or not to impute remaining missing values. Usually `True`\n",
    "\n",
    "**Note**: All NAs must be dropped or imputed to perform feature alignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DROP_COLS_THRESHOLD = None\n",
    "DROP_ROWS_THRESHOLD = None\n",
    "IMPUTE = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter by missing values\n",
    "mvf_metadata, mvf_features = missing_values_filter(\n",
    "    pf_metadata,\n",
    "    pf_features,\n",
    "    drop_cols_threshold=DROP_COLS_THRESHOLD,\n",
    "    drop_rows_threshold=DROP_ROWS_THRESHOLD,\n",
    "    impute=True,\n",
    ")\n",
    "\n",
    "mvf_metadata.shape, mvf_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'>SET PARAMETERS</font>\n",
    "\n",
    "### Intensity filtering\n",
    "\n",
    "- `CONTAMINATION`: Expected proportion of outliers in dataset. Usually `0.01`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTAMINATION = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter by intensity outliers\n",
    "if_metadata, if_features = intensity_filter(\n",
    "    mvf_metadata,\n",
    "    mvf_features,\n",
    "    config[\"phenotype\"][\"channel_names\"],\n",
    "    CONTAMINATION,\n",
    ")\n",
    "\n",
    "if_metadata.shape, if_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'>SET PARAMETERS</font>\n",
    "\n",
    "### Prepare alignment data\n",
    "\n",
    "- `BATCH_COLS`: Which columns of metadata have batch-specific information. Usually `[\"plate\", \"well\"]`.\n",
    "- `CONTROL_KEY`: Name of perturbation in `PERTURBATION_NAME_COL` that indicates a control cell.\n",
    "- `PERTURBATION_ID_COL`: Name of column that identifies unique perturbations. Only needed if you want your controls to have different perturbation names, ex `sgRNA_0`. Otherwise, can leave this as `None`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_COLS = None\n",
    "CONTROL_KEY = None\n",
    "PERTURBATION_ID_COL = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_metadata, pad_features = prepare_alignment_data(\n",
    "    if_metadata, if_features, BATCH_COLS, PERTURBATION_NAME_COL, CONTROL_KEY, PERTURBATION_ID_COL\n",
    ")\n",
    "\n",
    "n_components, fig = pca_variance_plot(\n",
    "    pad_features, variance_threshold=0.99\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'>SET PARAMETERS</font>\n",
    "\n",
    "### Align and aggregate\n",
    "\n",
    "- `VARIANCE_OR_NCOMP`: Variance or number of components to keep after PCA.\n",
    "- `NUM_ALIGN_BATCHES`: Number of batches to use when aligning, usually `1`. Increase this if you are running out of memory while aligning. We were able to barely fit 8 plates with 6 wells each in 1 TB of memory with `NUM_ALIGN_BATCHES=1`.\n",
    "- `AGG_METHOD`: Method used to aggregate features. Can be `mean` or `median`. Usually `median`.\n",
    "\n",
    "While we use a simplified aggregate method in the notebook, the way this works during a normal run is:\n",
    "1) Take a subset of 1,000,000 cells, or the entire dataset, whichever is smaller and compute a PCA transform with `VARIANCE_OR_NCOMP`.\n",
    "2) Subset the entire dataset `NUM_BATCHES` number of times and align cells in this batch.\n",
    "3) Aggregate across all aligned cell data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VARIANCE_OR_NCOMP = None\n",
    "NUM_ALIGN_BATCHES = None\n",
    "AGG_METHOD = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_embeddings = embed_by_pca(\n",
    "    pad_features, pad_metadata, variance_or_ncomp=VARIANCE_OR_NCOMP, batch_col=\"batch_values\"\n",
    ")\n",
    "\n",
    "tvn_normalized = tvn_on_controls(\n",
    "    pca_embeddings, pad_metadata, PERTURBATION_NAME_COL, CONTROL_KEY, \"batch_values\"\n",
    ")\n",
    "\n",
    "aggregated_embeddings, aggregated_metadata = aggregate(\n",
    "    tvn_normalized, pad_metadata, PERTURBATION_NAME_COL, AGG_METHOD\n",
    ")\n",
    "\n",
    "feature_columns = [f\"PC_{i}\" for i in range(tvn_normalized.shape[1])]\n",
    "\n",
    "tvn_normalized_df = pd.DataFrame(\n",
    "    tvn_normalized, index=pad_metadata.index, columns=feature_columns\n",
    ")\n",
    "aligned_cell_data = pd.concat([pad_metadata, tvn_normalized_df], axis=1)\n",
    "\n",
    "aggregated_embeddings_df = pd.DataFrame(\n",
    "    aggregated_embeddings, index=aggregated_metadata.index, columns=feature_columns\n",
    ")\n",
    "aggregated_cell_data = (\n",
    "    pd.concat([aggregated_metadata, aggregated_embeddings_df], axis=1)\n",
    "    .sort_values(\"cell_count\", ascending=False)\n",
    "    .reset_index(drop=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_feature_cols = [col for col in dataset_features.columns if (\"cell_\" in col and col.endswith(\"_mean\"))]\n",
    "pc_cols = [col for col in aggregated_cell_data.columns if col.startswith(\"PC_\")]\n",
    "aligned_feature_cols = random.sample(pc_cols, k=min(len(original_feature_cols), len(pc_cols)))\n",
    "\n",
    "original_cell_data = pd.concat([dataset_metadata, dataset_features], axis=1)\n",
    "original_cell_data\n",
    "\n",
    "feature_distributions_fig = plot_feature_distributions(\n",
    "    original_feature_cols,\n",
    "    original_cell_data,\n",
    "    aligned_feature_cols,\n",
    "    aligned_cell_data,\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add aggregate parameters to config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add aggregate section\n",
    "config[\"aggregate\"] = {\n",
    "    \"metadata_cols_fp\": METADATA_COLS_FP,\n",
    "    \"collapse_cols\": COLLAPSE_COLS,\n",
    "    \"classifier_path\": CLASSIFIER_PATH,\n",
    "    \"aggregate_combo_fp\": AGGREGATE_COMBO_FP,\n",
    "    \"filter_queries\": FILTER_QUERIES,\n",
    "    \"perturbation_name_col\": PERTURBATION_NAME_COL,\n",
    "    \"drop_cols_threshold\": DROP_COLS_THRESHOLD,\n",
    "    \"drop_rows_threshold\": DROP_ROWS_THRESHOLD,\n",
    "    \"impute\": IMPUTE,\n",
    "    \"contamination\": CONTAMINATION,\n",
    "    \"batch_cols\": BATCH_COLS,\n",
    "    \"control_key\": CONTROL_KEY,\n",
    "    \"perturbation_id_col\": PERTURBATION_ID_COL,\n",
    "    \"variance_or_ncomp\": VARIANCE_OR_NCOMP,\n",
    "    \"num_align_batches\": NUM_ALIGN_BATCHES,\n",
    "    \"agg_method\": AGG_METHOD,\n",
    "}\n",
    "\n",
    "# Write the updated configuration\n",
    "with open(CONFIG_FILE_PATH, \"w\") as config_file:\n",
    "    # Write the introductory comments\n",
    "    config_file.write(CONFIG_FILE_HEADER)\n",
    "\n",
    "    # Dump the updated YAML structure, keeping markdown comments for sections\n",
    "    yaml.dump(config, config_file, default_flow_style=False, sort_keys=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "brieflow_main_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
